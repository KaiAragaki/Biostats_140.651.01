---
title: "BST 140.651 Problem Set 2"
author: "Kai Aragaki"
date: "9/15/2020"
output: 
        tufte::tufte_html: default
        tufte::tufte_handout: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "output", output_format = "all") })

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Problem 1

-----------------------------------

$Var(X) = E[X^2]-E[X]^2$

Where $Var(X) = E[(X-\mu)^2]$, so

$E[(X-\mu)^2] = E[X^2]-E[X]^2$

$E[X^2-2X\mu + \mu^2] = E[X^2]-E[X]^2$

$E[X^2] + E[-2X\mu] + E[\mu^2] = E[X^2]-E[X]^2$

Since $\mu$ is a constant, and constants can be pulled out of the expected value function,

$E[X^2] -2\mu E[X] + \mu^2 E[1] = E[X^2] -E[X]^2$

Simplifying, and with the knowledge that $E[X] = \mu$:

$-2\mu^2 + \mu^2 = -\mu^2$

From which we find

$-2\mu^2 = -2\mu^2$

Which we know to be true.



# Problem 2

-----------------------------------

## a

-----------------------------------

$g$ is a valid density if all values are positive and if the integral across all values is 1.

We know all values are positive because it is the sum of three densities (which themselves must always be positive), each of which is multiplied by a scalar that is also positive.

To show that the integral across all values is equal to 1,

$\int\limits_{-\infty}^{\infty}g(x) = 1$

Since we know

$g(x) = \pi_1f_1(x) + \pi_2f_2(x) + \pi_3f_3(x)$

then

$\int\limits_{-\infty}^{\infty}g(x) = \int\limits_{-\infty}^{\infty}\Big(\pi_1f_1(x) + \pi_2f_2(x) + \pi_3f_3(x)\Big)$

which is equivalent to

$\int\limits_{-\infty}^{\infty}g(x) = \pi_1\int\limits_{-\infty}^{\infty}f_1(x) + \pi_2\int\limits_{-\infty}^{\infty}f_2(x)+ \pi_3\int\limits_{-\infty}^{\infty}f_3(x)$

and since each function $f_i$ is a valid density their integrals must equal 1, giving

$\int\limits_{-\infty}^{\infty}g(x) = \pi_1 + \pi_2 + \pi_3$

Since we are additionally told $\Sigma^3_{i=1}\pi_i = 1$

$\int\limits_{-\infty}^{\infty}g(x) = 1$
```{marginfigure}
$\blacksquare$
```


## b

-----------------------------------

We know that mean (or expected value) for a continous distribution is given by

$\int\limits_{-\infty}^{\infty}xg(x)$

Which is

$\int\limits_{-\infty}^{\infty}x\Big(\pi_1f_1(x)+\pi_2f_2(x)+\pi_3f_3(x)\Big)$

which can be expanded to 

$\pi_1\int\limits_{-\infty}^{\infty}xf_1(x) + \pi_2\int\limits_{-\infty}^{\infty}xf_2(x)+\pi_3\int\limits_{-\infty}^{\infty}xf_3(x)$

And since we know the definition of the expected value, we can rewrite this as

$E[X] = \pi_1\mu_1 + \pi_2\mu_2+\pi_3\mu_3$


## c

-----------------------------------

We have previously shown that the definition of 

$Var(x) = E[X^2] - E[X]^2$

We can represent the variance of $g$ in these terms:

$\int\limits_{-\infty}^{\infty}x^2g(x) - [\int\limits_{-\infty}^{\infty}xg(x)]^2$

The right term we have previously found - it's simply the square of the expectation value we found in the previous problem. The left terms can be found by first writing $g$ in terms of $f$

$\int\limits_{-\infty}^{\infty}x^2g(x) = \int\limits_{-\infty}^{\infty}x^2\Big(\pi_1f_1(x)+\pi_2f_2(x)+\pi_3f_3(x)\Big)$

or distributed:

$\int\limits_{-\infty}^{\infty}x^2g(x) = \pi_1\int\limits_{-\infty}^{\infty}x^2f_1(x) + \pi_2\int\limits_{-\infty}^{\infty}x^2f_2(x) + \pi_3\int\limits_{-\infty}^{\infty}x^2f_3(x)$


and then noticing that the variance equation can be written as follows:

$\sigma^2 = E[X^2] + \mu^2$

And solving for $E[X^2]$:

$E[X^2] = \mu^2 + \sigma^2$

which is equivalent to

$\int\limits_{-\infty}^{\infty}x^2f_i(x) = \mu^2_i + \sigma^2_i$

Substituting in, we get

$\int\limits_{-\infty}^{\infty}x^2g(x) = \pi_1(\mu^2_1 + \sigma^2_1) +\pi_2(\mu^2_2 + \sigma^2_2) +\pi_3(\mu^2_3 + \sigma^2_3)$

Combining this left hand term with the right hand term, we get

$Var(x) = \pi_1(\mu^2_1 + \sigma^2_1) +\pi_2(\mu^2_2 + \sigma^2_2) +\pi_3(\mu^2_3 + \sigma^2_3) -(\pi_1\mu_1 + \pi_2\mu_2+\pi_3\mu_3)^2$


# Problem 3

-----------------------------------

## a

-----------------------------------

The mean of a continuous distribution is 

$\int\limits_{lower}^{upper}xf(x)$

In this case the lower and upper bounds are given as 0 and 1 respectively.

Therefore the mean is given by

$\int\limits_{0}^{1}(k+1)x^{k+1}$

$\int\limits_{0}^{1}kx^{k+1} + \int\limits_{0}^{1}x^{k+1}$

$\frac{kx^{k+2}}{k+2}\Bigg|^1_0 + \frac{x^{k+2}}{k+2}\Bigg|^1_0$

$\frac{k}{k+2} + \frac{1}{k+2}$

$\frac{k+1}{k+2}$


## b

-----------------------------------

The variance of this distribution is

$Var(x) = E[X^2] - E[X]^2$

We know $E[X]=\mu$, which is what we found in part $a$, so squaring it is trivial. Additionally,

$E[X^2] = \int\limits_{0}^{1}x^2\big((k+1)x^k\big) = \int\limits_{0}^{1}(k+1)x^{k+2}$

And by distributing, 

$\int\limits_{0}^{1}kx^{k+2} + \int\limits_{0}^{1}x^{k+2}$

We can see by analogy to the first problem that this will integrate and simplify to

$\frac{k+1}{k+3}$


# Problem 4

-----------------------------------

## a

-----------------------------------

To calculate the mean, we know that the mean is, for the limits of this given function 

$E[X] = \int\limits^\infty_{0}xf(x) = \frac{1}{3.3}\int\limits^\infty_{0}xe^{\frac{-x}{3.3}}$

We can integrate by parts using - that is, we know the following to be true:

$\int udv = uv - \int vdu$

so if we set $u = x$, $du = dx$, $v = -3.3e^{\frac{-x}{3.3}}$, and $dv = e^{\frac{-x}{3.3}}dx$,

$\int xe^{\frac{-x}{3.3}}= 3.3xe^{\frac{-x}{3.3}} + 3.3\int e^{\frac{-x}{3.3}}dx = 3.3xe^{\frac{-x}{3.3}} -  3.3^2e^{\frac{-x}{3.3}}$

Substituting this integral back into the original, we obtain

$xe^{\frac{-x}{3.3}} -  3.3e^{\frac{-x}{3.3}}$

evaluating at its limits of $0$ and $\infty$, and with the knowledge that $e^{-x}$ approaches 0 much faster than $x$ approaches infinity,

$xe^{\frac{-x}{3.3}} - 3.3e^{\frac{-x}{3.3}}\Big|^\infty_0 = [0 + 0] - [0 - 3.3] = 3.3$
```{marginfigure}
$\blacksquare$
```

To calculate the variance, we solve the following:

$Var(X) = \int\limits^\infty_{0}x^2f(x) - E[X]^2$ 

But since we know the value of $E[X]$ (and squaring it is trivial) we focus our efforts on the left hand side of the equation:

$\int\limits^\infty_{0}x^2f(x) = \frac{1}{3.3}\int\limits^\infty_{0}x^2e^{\frac{-x}{3.3}}$

As before, we begin to integrate by parts, with $u = x^2$, $du = 2xdx$, $v = -3.3e^{\frac{-x}{3.3}}$, and $dv = e^{\frac{-x}{3.3}}$:

$\frac{1}{3.3}[-3.3x^2e^{\frac{-x}{3.3}}+6.6\int e^{\frac{-x}{3.3}}xdx]$

We integrate by parts again, this time where $u = x$, $du = dx$, $v = -3.3e^{\frac{-x}{3.3}}$, and $dv = e^{\frac{-x}{3.3}}dx$,

$\frac{1}{3.3}\Big[-3.3x^2e^{\frac{-x}{3.3}}+6.6[-3.3xe^{\frac{-x}{3.3}} + 3.3 \int e^{\frac{-x}{3.3}}dx]\Big]$

Which is equivalent to

$\frac{1}{3.3}\Big[-3.3x^2e^{\frac{-x}{3.3}}+6.6[-3.3xe^{\frac{-x}{3.3}} - 3.3^2 e^{\frac{-x}{3.3}}]\Big]$

As before, evaluating at the limits $0$ to $\infty$ and substitute back in $E[X]^2$

$Var(X) = 2(3.3)^2-3.3^2 = 3.3^2 = 10.89$
```{marginfigure}
$\blacksquare$
```

## b

-----------------------------------

If we substitute $3.3$ for $\beta$ in our previous solutions, we find

$E[X] = xe^{\frac{-x}{\beta}} - \beta e^{\frac{-x}{\beta}}\Big|^\infty_0 = [0 + 0] - [0 - \beta] = \beta$

and 

$Var(X) = \frac{1}{\beta}\Big[-\beta x^2e^{\frac{-x}{\beta}}+2\beta[-\beta xe^{\frac{-x}{\beta}} - \beta^2 e^{\frac{-x}{\beta}}]\Big] - \beta^2 = \beta^2$

## c

-----------------------------------

```{r}
tib <- expand_grid(x = seq(0, 10, by = 0.01),
              b = c(0.1, 1, 10)) %>% 
        mutate(y = (1/b)*exp(-x/b))

ggplot(tib, aes(x = x, y = y, color = as.factor(b))) + scale_color_viridis_d(end = 0.8) + geom_line()
```

# Problem 5

-----------------------------------

## a

-----------------------------------

We know the mean to be 

$\int\limits_0^\infty xf(x)dx = \int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}x^\alpha e^{\frac{-x}{\beta}}dx$

We begin by substituting, where $u = \frac{x}{\beta}$ and $du = \frac{dx}{\beta}$, which implies $x = \beta u$ and $dx = \beta du$:

$\int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}(\beta u)^\alpha e^{-u}\beta du = \int\limits_0^\infty \frac{1}{\Gamma(\alpha)}u^\alpha  e^{-u}\beta du = \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\beta$

Since we know that $\Gamma(\alpha) = (\alpha-1)!$, then

$\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\beta = \alpha\beta$
```{marginfigure}
$\blacksquare$
```

For the variance, we know

$Var(X) = E[X^2] - E[X]^2$

We begin to solve for the first portion:

$E[X^2] = \int \limits _0^\infty x^2f(x) = \int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha+1} e^{\frac{-x}{\beta}}dx$

Using the same substitution as above, we can write it as

$\int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}(\beta u)^{\alpha+1} e^u \beta du = \int\limits_0^\infty \frac{1}{\Gamma(\alpha)}\beta^2u^{\alpha+1} e^udu = \frac{\Gamma(\alpha+2)}{\Gamma(\alpha)}\beta^2 = (\alpha + 1)\alpha\beta^2 = \alpha^2\beta^2 + \alpha\beta^2$

Solving for second portion is as simple as squaring the value we found for the mean, so in total,

$Var(X) = E[X^2] - E[X]^2 = \alpha^2\beta^2 + \alpha\beta^2 - \alpha^2\beta^2 = \alpha\beta^2$
```{marginfigure}
$\blacksquare$
```

## b

-----------------------------------

We can simply substitute in these values for $\alpha$ and $\beta$:

$\mu = \alpha\beta = \frac{2p}{2} = p$

$\sigma^2 = \alpha\beta^2 = \frac{4p}{2} = 2p$


