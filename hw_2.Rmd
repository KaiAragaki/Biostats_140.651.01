---
title: "BST 140.651 Problem Set 2"
author: "Kai Aragaki"
date: "9/15/2020"
output: 
        tufte::tufte_html: default
        tufte::tufte_handout: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "output", output_format = "all") })

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Problem 1

-----------------------------------

$Var(X) = E[X^2]-E[X]^2$

Where $Var(X) = E[(X-\mu)^2]$, so

$E[(X-\mu)^2] = E[X^2]-E[X]^2$

$E[X^2-2X\mu + \mu^2] = E[X^2]-E[X]^2$

$E[X^2] + E[-2X\mu] + E[\mu^2] = E[X^2]-E[X]^2$

Since $\mu$ is a constant, and constants can be pulled out of the expected value function,

$E[X^2] -2\mu E[X] + \mu^2 E[1] = E[X^2] -E[X]^2$

Simplifying, and with the knowledge that $E[X] = \mu$:

$-2\mu^2 + \mu^2 = -\mu^2$

From which we find

$-2\mu^2 = -2\mu^2$

Which we know to be true.



# Problem 2

-----------------------------------

## a

-----------------------------------

$g$ is a valid density if all values are positive and if the integral across all values is 1.

We know all values are positive because it is the sum of three densities (which themselves must always be positive), each of which is multiplied by a scalar that is also positive.

To show that the integral across all values is equal to 1,

$\int\limits_{-\infty}^{\infty}g(x) = 1$

Since we know

$g(x) = \pi_1f_1(x) + \pi_2f_2(x) + \pi_3f_3(x)$

then

$\int\limits_{-\infty}^{\infty}g(x) = \int\limits_{-\infty}^{\infty}\Big(\pi_1f_1(x) + \pi_2f_2(x) + \pi_3f_3(x)\Big)$

which is equivalent to

$\int\limits_{-\infty}^{\infty}g(x) = \pi_1\int\limits_{-\infty}^{\infty}f_1(x) + \pi_2\int\limits_{-\infty}^{\infty}f_2(x)+ \pi_3\int\limits_{-\infty}^{\infty}f_3(x)$

and since each function $f_i$ is a valid density their integrals must equal 1, giving

$\int\limits_{-\infty}^{\infty}g(x) = \pi_1 + \pi_2 + \pi_3$

Since we are additionally told $\Sigma^3_{i=1}\pi_i = 1$

$\int\limits_{-\infty}^{\infty}g(x) = 1$
```{marginfigure}
$\blacksquare$
```


## b

-----------------------------------

We know that mean (or expected value) for a continous distribution is given by

$\int\limits_{-\infty}^{\infty}xg(x)$

Which is

$\int\limits_{-\infty}^{\infty}x\Big(\pi_1f_1(x)+\pi_2f_2(x)+\pi_3f_3(x)\Big)$

which can be expanded to 

$\pi_1\int\limits_{-\infty}^{\infty}xf_1(x) + \pi_2\int\limits_{-\infty}^{\infty}xf_2(x)+\pi_3\int\limits_{-\infty}^{\infty}xf_3(x)$

And since we know the definition of the expected value, we can rewrite this as

$E[X] = \pi_1\mu_1 + \pi_2\mu_2+\pi_3\mu_3$


## c

-----------------------------------

We have previously shown that the definition of 

$Var(x) = E[X^2] - E[X]^2$

We can represent the variance of $g$ in these terms:

$\int\limits_{-\infty}^{\infty}x^2g(x) - [\int\limits_{-\infty}^{\infty}xg(x)]^2$

The right term we have previously found - it's simply the square of the expectation value we found in the previous problem. The left terms can be found by first writing $g$ in terms of $f$

$\int\limits_{-\infty}^{\infty}x^2g(x) = \int\limits_{-\infty}^{\infty}x^2\Big(\pi_1f_1(x)+\pi_2f_2(x)+\pi_3f_3(x)\Big)$

or distributed:

$\int\limits_{-\infty}^{\infty}x^2g(x) = \pi_1\int\limits_{-\infty}^{\infty}x^2f_1(x) + \pi_2\int\limits_{-\infty}^{\infty}x^2f_2(x) + \pi_3\int\limits_{-\infty}^{\infty}x^2f_3(x)$


and then noticing that the variance equation can be written as follows:

$\sigma^2 = E[X^2] + \mu^2$

And solving for $E[X^2]$:

$E[X^2] = \mu^2 + \sigma^2$

which is equivalent to

$\int\limits_{-\infty}^{\infty}x^2f_i(x) = \mu^2_i + \sigma^2_i$

Substituting in, we get

$\int\limits_{-\infty}^{\infty}x^2g(x) = \pi_1(\mu^2_1 + \sigma^2_1) +\pi_2(\mu^2_2 + \sigma^2_2) +\pi_3(\mu^2_3 + \sigma^2_3)$

Combining this left hand term with the right hand term, we get

$Var(x) = \pi_1(\mu^2_1 + \sigma^2_1) +\pi_2(\mu^2_2 + \sigma^2_2) +\pi_3(\mu^2_3 + \sigma^2_3) -(\pi_1\mu_1 + \pi_2\mu_2+\pi_3\mu_3)^2$


# Problem 3

-----------------------------------

## a

-----------------------------------

The mean of a continuous distribution is 

$\int\limits_{lower}^{upper}xf(x)$

In this case the lower and upper bounds are given as 0 and 1 respectively.

Therefore the mean is given by

$\int\limits_{0}^{1}(k+1)x^{k+1}$

$\int\limits_{0}^{1}kx^{k+1} + \int\limits_{0}^{1}x^{k+1}$

$\frac{kx^{k+2}}{k+2}\Bigg|^1_0 + \frac{x^{k+2}}{k+2}\Bigg|^1_0$

$\frac{k}{k+2} + \frac{1}{k+2}$

$\frac{k+1}{k+2}$


## b

-----------------------------------

The variance of this distribution is

$Var(x) = E[X^2] - E[X]^2$

We know $E[X]=\mu$, which is what we found in part $a$, so squaring it is trivial. Additionally,

$E[X^2] = \int\limits_{0}^{1}x^2\big((k+1)x^k\big) = \int\limits_{0}^{1}(k+1)x^{k+2}$

And by distributing, 

$\int\limits_{0}^{1}kx^{k+2} + \int\limits_{0}^{1}x^{k+2}$

We can see by analogy to the first problem that this will integrate and simplify to

$\frac{k+1}{k+3}$


# Problem 4

-----------------------------------

## a

-----------------------------------

To calculate the mean, we know that the mean is, for the limits of this given function 

$E[X] = \int\limits^\infty_{0}xf(x) = \frac{1}{3.3}\int\limits^\infty_{0}xe^{\frac{-x}{3.3}}$

We can integrate by parts using - that is, we know the following to be true:

$\int udv = uv - \int vdu$

so if we set $u = x$, $du = dx$, $v = -3.3e^{\frac{-x}{3.3}}$, and $dv = e^{\frac{-x}{3.3}}dx$,

$\int xe^{\frac{-x}{3.3}}= 3.3xe^{\frac{-x}{3.3}} + 3.3\int e^{\frac{-x}{3.3}}dx = 3.3xe^{\frac{-x}{3.3}} -  3.3^2e^{\frac{-x}{3.3}}$

Substituting this integral back into the original, we obtain

$xe^{\frac{-x}{3.3}} -  3.3e^{\frac{-x}{3.3}}$

evaluating at its limits of $0$ and $\infty$, and with the knowledge that $e^{-x}$ approaches 0 much faster than $x$ approaches infinity,

$xe^{\frac{-x}{3.3}} - 3.3e^{\frac{-x}{3.3}}\Big|^\infty_0 = [0 + 0] - [0 - 3.3] = 3.3$
```{marginfigure}
$\blacksquare$
```

To calculate the variance, we solve the following:

$Var(X) = \int\limits^\infty_{0}x^2f(x) - E[X]^2$ 

But since we know the value of $E[X]$ (and squaring it is trivial) we focus our efforts on the left hand side of the equation:

$\int\limits^\infty_{0}x^2f(x) = \frac{1}{3.3}\int\limits^\infty_{0}x^2e^{\frac{-x}{3.3}}$

As before, we begin to integrate by parts, with $u = x^2$, $du = 2xdx$, $v = -3.3e^{\frac{-x}{3.3}}$, and $dv = e^{\frac{-x}{3.3}}$:

$\frac{1}{3.3}[-3.3x^2e^{\frac{-x}{3.3}}+6.6\int e^{\frac{-x}{3.3}}xdx]$

We integrate by parts again, this time where $u = x$, $du = dx$, $v = -3.3e^{\frac{-x}{3.3}}$, and $dv = e^{\frac{-x}{3.3}}dx$,

$\frac{1}{3.3}\Big[-3.3x^2e^{\frac{-x}{3.3}}+6.6[-3.3xe^{\frac{-x}{3.3}} + 3.3 \int e^{\frac{-x}{3.3}}dx]\Big]$

Which is equivalent to

$\frac{1}{3.3}\Big[-3.3x^2e^{\frac{-x}{3.3}}+6.6[-3.3xe^{\frac{-x}{3.3}} - 3.3^2 e^{\frac{-x}{3.3}}]\Big]$

As before, evaluating at the limits $0$ to $\infty$ and substitute back in $E[X]^2$

$Var(X) = 2(3.3)^2-3.3^2 = 3.3^2 = 10.89$
```{marginfigure}
$\blacksquare$
```

## b

-----------------------------------

If we substitute $3.3$ for $\beta$ in our previous solutions, we find

$E[X] = xe^{\frac{-x}{\beta}} - \beta e^{\frac{-x}{\beta}}\Big|^\infty_0 = [0 + 0] - [0 - \beta] = \beta$

and 

$Var(X) = \frac{1}{\beta}\Big[-\beta x^2e^{\frac{-x}{\beta}}+2\beta[-\beta xe^{\frac{-x}{\beta}} - \beta^2 e^{\frac{-x}{\beta}}]\Big] - \beta^2 = \beta^2$

## c

-----------------------------------

```{r}
tib <- expand_grid(x = seq(0, 10, by = 0.01),
              b = c(0.1, 1, 10)) %>% 
        mutate(y = (1/b)*exp(-x/b))

ggplot(tib, aes(x = x, y = y, color = as.factor(b))) + scale_color_viridis_d(end = 0.8) + geom_line()
```

# Problem 5

-----------------------------------

## a

-----------------------------------

We know the mean to be 

$\int\limits_0^\infty xf(x)dx = \int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}x^\alpha e^{\frac{-x}{\beta}}dx$

We begin by substituting, where $u = \frac{x}{\beta}$ and $du = \frac{dx}{\beta}$, which implies $x = \beta u$ and $dx = \beta du$:

$\int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}(\beta u)^\alpha e^{-u}\beta du = \int\limits_0^\infty \frac{1}{\Gamma(\alpha)}u^\alpha  e^{-u}\beta du = \frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\beta$

Since we know that $\Gamma(\alpha) = (\alpha-1)!$, then

$\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\beta = \alpha\beta$
```{marginfigure}
$\blacksquare$
```

For the variance, we know

$Var(X) = E[X^2] - E[X]^2$

We begin to solve for the first portion:

$E[X^2] = \int \limits _0^\infty x^2f(x) = \int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha+1} e^{\frac{-x}{\beta}}dx$

Using the same substitution as above, we can write it as

$\int\limits_0^\infty \frac{1}{\beta^\alpha\Gamma(\alpha)}(\beta u)^{\alpha+1} e^u \beta du = \int\limits_0^\infty \frac{1}{\Gamma(\alpha)}\beta^2u^{\alpha+1} e^udu = \frac{\Gamma(\alpha+2)}{\Gamma(\alpha)}\beta^2 = (\alpha + 1)\alpha\beta^2 = \alpha^2\beta^2 + \alpha\beta^2$

Solving for second portion is as simple as squaring the value we found for the mean, so in total,

$Var(X) = E[X^2] - E[X]^2 = \alpha^2\beta^2 + \alpha\beta^2 - \alpha^2\beta^2 = \alpha\beta^2$
```{marginfigure}
$\blacksquare$
```

## b

-----------------------------------

We can simply substitute in these values for $\alpha$ and $\beta$:

$\mu = \alpha\beta = \frac{2p}{2} = p$

$\sigma^2 = \alpha\beta^2 = \frac{4p}{2} = 2p$


# Problem 6

-----------------------------------

## a

-----------------------------------

$E[X] = \int \limits_0^1xf(x) = \int \limits_0^1 \frac{x^\alpha(1-x)^{\beta-1}}{B(\alpha, \beta)}$

since we know

$B(\alpha, \beta) = \int\limits^1_0 x^{\alpha-1}(1-x)^{\beta-1}dx = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}$

then

$B(\alpha + 1, \beta) = \int\limits^1_0 x^{\alpha}(1-x)^{\beta-1}dx = \frac{\Gamma(\alpha + 1)\Gamma(\beta)}{\Gamma(\alpha + 1 + \beta)}$

so

$\int \limits_0^1 \frac{x^\alpha(1-x)^{\beta-1}}{B(\alpha, \beta)} = \frac{\Gamma(\alpha + 1)\Gamma(\beta)}{\Gamma(\alpha + 1 + \beta)} \times \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} = \frac{\alpha\Gamma(\alpha)\Gamma(\beta)}{(\alpha + \beta)\Gamma(\alpha + \beta)} \times \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} = \frac{\alpha}{\alpha + \beta}$
```{marginfigure}
$\blacksquare$
```

## b

-----------------------------------

For the variance, like above we solve in portions, starting with the first portion:

$E[X^2] = \int \limits_0^1x^2f(x) = \int \limits_0^1 \frac{x^{\alpha+1}(1-x)^{\beta-1}}{B(\alpha, \beta)} = \frac{\Gamma(\alpha + 2)\Gamma(\beta)}{\Gamma(\alpha + 2 + \beta)} \times \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} = \frac{(\alpha + 1)(\alpha)\Gamma(\alpha)\Gamma(\beta)}{(\alpha + \beta + 1)(\alpha+\beta)\Gamma(\alpha + \beta)} \times \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} = \frac{\alpha(\alpha+1)}{(\alpha + \beta + 1)(\alpha + \beta)}$

The second portion is simply the mean squared, so the variance becomes

$Var(X) = \frac{\alpha(\alpha+1)}{(\alpha + \beta + 1)(\alpha + \beta)} - \Big(\frac{\alpha}{\alpha + \beta}\Big)^2 = \frac{\alpha(\alpha+1)(\alpha + \beta) - (\alpha + \beta + 1)\alpha^2}{(\alpha + \beta + 1)(\alpha + \beta)^2} = \frac{\alpha\beta}{(\alpha + \beta + 1)(\alpha + \beta)^2}$
```{marginfigure}
$\blacksquare$
```

# Problem 7

-----------------------------------

## a

-----------------------------------

The mean for discrete functions can be written as

$E[X] = \sum\limits_{x = 0}^{\infty}xp(x) = \sum\limits_{x = 0}^{\infty}\frac{xe^{-\lambda}\lambda^x}{x!}$

We know $e^{-\lambda}$ is constant, so we can pull it out of the sum:

$e^{-\lambda}\sum\limits_{x = 0}^{\infty}\frac{x\lambda^x}{x!}$

We further note that the first term is 0 - so we can safely start the sum at $x = 1$:

$e^{-\lambda}\sum\limits_{x = 1}^{\infty}\frac{x\lambda^x}{x!}$

We now note two things: Each term in our summation has at least one $\lambda$, which can be factored out. Additionally, $\frac{x}{x!} = \frac{1}{(x-1)!}$, so the sum can be rewritten as

$e^{-\lambda}\lambda\sum\limits_{x = 1}^{\infty}\frac{\lambda^{x-1}}{(x-1)!}$

By performing a change of variable, where $y = x - 1$, we can more easily see a familiar infinite sum:

$e^{-\lambda}\lambda\sum\limits_{y = 1}^{\infty}\frac{\lambda^{y}}{y!} = e^{-\lambda}e^\lambda\lambda = \lambda$
```{marginfigure}
$\blacksquare$
```

## b

-----------------------------------

For this problem, we are given a hint of $E[X(X-1)]$. Let's evaluate this:

$E[X(X-1)] = \sum\limits_{x=0}^\infty \frac{x(x-1)e^{-\lambda}\lambda^x}{x!}$ 

We initially note that the first two terms will be 0, so we can shift our sum to start at 2:

$\sum\limits_{x=2}^\infty \frac{x(x-1)e^{-\lambda}\lambda^x}{x!}$ 

We also note again that $e^{-\lambda}$ is constant and can be factored out:

$e^{-\lambda}\sum\limits_{x=2}^\infty \frac{x(x-1)\lambda^x}{x!}$

Further still, we note that the first two terms of $x!$ can be cancelled out with the terms in the numerator:

$e^{-\lambda}\sum\limits_{x=2}^\infty \frac{\lambda^x}{(x-2)!}$

If we examine the first few terms, we will notice that each term has at least $\lambda^2$. We can factor this out as well:

$e^{-\lambda}\lambda^2\sum\limits_{x=2}^\infty \frac{\lambda^{x-2}}{(x-2)!}$

A change of variable (where $y = x-2$) lets us see that this is a familiar sum:

$e^{-\lambda}\lambda^2\sum\limits_{y=0}^\infty \frac{\lambda^{y}}{(y)!} = e^{-\lambda}e^\lambda\lambda^2 = \lambda^2$

We know this to be $E[X(X-1)]$. Manipulating this expression, we see

$E[X(X-1)] = E[X^2-X]$

Since we know expectation values can be separated, this is equivalent to 

$E[X^2] - E[X]$

and we know $E[X] = \lambda$
So we can solve for $E[X^2]$:

$\lambda^2 = E[X^2] - \lambda \implies E[X^2] = \lambda^2 + \lambda$

So,

$Var(X) =\lambda^2 + \lambda - E[X]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$
```{marginfigure}
$\blacksquare$
```

# Problem 8

-----------------------------------

## a

-----------------------------------

We can write the uniform distribution as

$f(x) = 1$

So the expected value (for $0 < x < 1$) can be calculated as

$E[X] = \int\limits^1_0 xdx = \frac{x^2}{2}\Big|^1_0 = \frac{1}{2}$

Converting to a percentage, the expected percentage of a randomly selected person's body that is covered in freckles is $50\%$

## b

-----------------------------------

The variance is given by

$Var(X) = E[X^2] - E[X]^2$

Beginning with the first portion,

$E[X^2] = \int\limits^1_0 x^2dx = \frac{x^3}{3}\Big|^1_0 = \frac{1}{3}$

So,

$Var(X) = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}$

# Problem 9

-----------------------------------

## a

-----------------------------------

If we consider $X$ to be the number of songs listened to AFTER the repeated song was listened to, then we know that $P(X = 1) = 0$ - listening to just one song cannot be a repeat of anything.

If we manage to listen to all the songs without a repeat, the next song we listen to MUST be a repeat: there is no song in the list that exists that we have not yet heard $P(X = 1001) = 1$. Any additional songs are not considered. So, $2 \le X \le 1001$.

Let's think about what the probability of listening to 3 songs (and hearing a repeat on the third song) is.

The probability of the first song not being a repeat is quite good, so the probability of this event happening is 1. The probability of the seoncd song not being a repeat is also quite good, but one of the songs is forbidden. The probability of having picked a song not yet played before is therefore $999/1000$. The third song is where we picked a song we have already heard before. The probability of that happening is quite slim - $2/1000$. Altogether, $P(X = 3) = \frac{1000}{1000}\frac{999}{1000}\frac{2}{1000}$

This is all well and good, but suppose we want to generalize this equation for $x$. Say we listened to $x$ songs, where the $x$th song was the duplicate. To generalize this, we can write

$P(X=x) = \frac{1000}{1000}\frac{999}{1000}...\frac{1000-(x+2)}{1000}\frac{x-1}{1000}$

The denominator can be simplified to $1000^x$, and the top reeks of a factorial...to me it looks very much like the front end of a factorial. And the front end of a factorial is simply a factorial with the back end divided out. So the numerator is something like

$1000*999*998*...*1000-(x+2) = 1000!/(1000-(x+1))!$

So altogether it becomes

$P(X = x) = \frac{1000!(x-1)}{1000^x(1000-(x+1))!}$


## b

-----------------------------------

It is much more computationally feasible the expected value if we define the probability in a recursive form.

We can think of the question we want to answer as "what is the probability of getting a repeat on this particular song number as well as not getting a repeat on the previous songs? Mathematically:

$P(X = k \cap X > k-1)$

We know that 

$P(A|B) = \frac{P(A\cap B)}{P(B)}$

by definition, so

$P(A\cap B) = P(A|B)P(B)$

so

$P(X = k \cap X > k-1) = P(X = k|X > k-1)P(X > k-1)$

We know that 

$P(X = k|X > k-1)$

is just

$\frac{k-1}{1000}$

and

$P(X > k-1)$

is 

$1-(P(X=k-1) + P(X = k-2) + ... + P(X = 2))$

So in total,

$\frac{k-1}{1000}(1-(P(X=k-1) + P(X = k-2) + ... + P(X = 2)))$

Thus we can define each probability incrementally instead of requiring calculating $1000!$, which can in turn be implemented into a loop. These values can then be solved computationally to receive approximate values rather than exact:

```{r}
# Code shamefully stolen from the lab markdown
nsongs <- 1000

xk <- 1:(nsongs+1)

pk <- rep(0, nsongs+1)

for(k in 1:length(xk)){
    pk[k] <- (k-1)/nsongs * (1-sum(pk[1:(k-1)]))
}

EX <-sum(pk * xk)
VX <- sum(pk * xk^2) - EX^2
```

# Problem 11

-----------------------------------


This is essentially asking for a quantile of a geometric distribution. `qgeom` will give us the number of 'failures' (not contracting HIV) before a 'success' (contracting HIV) (Obviously these are just how they are coded - no one would consider contracting HIV to be an actual success). 

```{r}
qgeom(0.1, 1/500)
```
We will have 52 'failures' before we have a 0.1 cumulative probability of 'success' - that is, 53 acts of intercourse are required. Some assumptions of the geometric distribution: the events are independent - that is, one event's probabilities does not affect the probability of the other - and that there is no gradient of success and failure ie it is a success/failure binary.

# Problem 16

-----------------------------------

## a

-----------------------------------

For $M = 0, P(X = 0) = (408)/7745 \approx 0.053$
For $M = 1, P(X = 1) = (429 + 451 + 456 + 441)/7745 \approx 0.229$
For $M = 2, P(X = 2) = (497+486+473+526+498+490)/7745 \approx 0.384$
For $M = 3, P(X = 3) = (549+514+523+467)/7745 \approx 0.265$
For $M = 4, P(X = 4) = (537)/7745 \approx 0.069$

## b

-----------------------------------

This is the sum of each probability multiplied by X, so

$E[X] = 0*0.053 + 1*0.229 + 2*0.384 + 3*0.265 + 4*0.069 = 2.068$

## c

-----------------------------------

The variance is given by

$Var(X) = E[X^2] - E[X]^2$

$E[X^2] = 0^2*0.053 + 1^2*0.229 + 2^2*0.384 + 3^2*0.265 + 4^2*0.069 = 5.254$
$E[X]^2 = 2.068^2 = 4.277$

$Var(X) = 5.254 - 4.277 = 0.977$

## d

-----------------------------------

The conversion from number to proportion is as simple as converting $X = 0, 1, 2, 3, 4$ to $X = 0, 0.25, 0.5, 0.75, 1$

So

$E[X] = 0*0.053 + 0.25*0.229 + 0.5*0.384 + 0.75*0.265 + 1*0.069 = 0.517$

Which we note is simply the previous mean divided by 4

and

$Var(X) = E[X^2] - E[X]^2$

$E[X^2] = 0^2*0.053 + 0.25^2*0.229 + 0.5^2*0.384 + 0.75^2*0.265 + 1^2*0.069 = .328375$
$E[X]^2 = 0.517^2 = 0.267289$

$Var(X) = .328375 - 0.267289 = 0.061086$

Which we note is simply the previous variance divided by 16

# Problem 17

-----------------------------------

## a

-----------------------------------

$E[X] = \int \limits_1^\infty x\frac{3}{x^4} = 3\int \limits_1^\infty \frac{1}{x^3} = -\frac{3}{2}x^{-2}\Big|^\infty_1 = \frac{3}{2}$

The average failure time is 1.5 years

## b

-----------------------------------

$Var(X) = E[X^2] - E[X]^2$

$E[X^2] = \int \limits_1^\infty x^2\frac{3}{x^4} = 3\int \limits_1^\infty \frac{1}{x^2} = -3x^{-1}\Big|^\infty_1 = 3$

$Var(X) = 3 - 1.5^2 = 0.75$

The variance is 0.75 years^2

## c

-----------------------------------
The general form of the Pareto distribution is given by

$\frac{\beta\alpha^\beta}{x^{\beta+1}}$

$E[X] = \int \limits_\alpha^\infty x\frac{\beta\alpha^\beta}{x^{\beta+1}} = \beta\alpha^\beta\int \limits_\alpha^\infty \frac{1}{x^{\beta}} = -\frac{\beta\alpha^\beta x^{1-\beta}}{1-\beta}\Big|^\infty_\alpha = \frac{\beta\alpha}{\beta-1}$

$E[X^2] = \int \limits_\alpha^\infty x^2\frac{\beta\alpha^\beta}{x^{\beta+1}} = \beta\alpha^\beta\int \limits_\alpha^\infty \frac{1}{x^{\beta-1}} = \frac{\beta\alpha^\beta x^{-\beta+2}}{-\beta+2}\Big|^\infty_\alpha = \frac{\beta\alpha^2}{\beta-2}$

$Var(X) = E[X^2] - E[X]^2 = \frac{\beta\alpha^2}{\beta-2} - \Big(\frac{\beta\alpha}{\beta-1}\Big)^2$

# Problem 18

-----------------------------------

## a

-----------------------------------

The expected value can be calculated as 

$E[X] = Win*Prob + Loss*(1-Prob)$

Where in this instance we assume a fair coin ($Prob = 0.5$) that has flips that are independent (the outcome of one does not effect the outcome of the other) and there can only be two outcomes - heads or tails.

In this example, $Win  = \$1$ and $Loss = -\$1$, such that

$E[X] = \$1* 0.5  -\$1*0.5 = 0$

On a long enough timeframe, regardless of the number of tosses, we expect the true mean to be 0 - we will neither win nor lose.

## b

-----------------------------------

$Var(X) = E[X^2] - E[X]^2$

$E[X^2] = \$1^2*0.5+(-\$1)^2*0.5 = \$^21$

$E[X]^2 = 0$

$Var(X) = \$^21$

## c

-----------------------------------

In the case that $p = 0.4$,

$E[X] = \$1* 0.4  -\$1*0.6 = -\$0.2$

So in the case of 10 games, assuming they are iid, we will lose $2.

$E[X^2] = \$1^2*0.4+(-\$1)^2*0.6 = \$^21$

$E[X]^2 = \$^20.04$

$Var(X) = \$^20.96$


# Problem 19

-----------------------------------

## a

-----------------------------------

```{r}
temp <- matrix(sample(1:6, 1000*10, replace = T), 1000)
xBar <- apply(temp, 1, mean)
```

```{r}
hist(xBar, breaks = 100)
```

## b

-----------------------------------

```{r}
mean(xBar)
```
The mean of `xBar` is close to the expectation value for a die. We would expect this to be true since the expectation value for a die is indeed 3.5. When we we take samples with replacement from any distribution and take the means, the means tend to be normally distributed, and the mean of means tends to be centered on the mean of the original population. This is due to the central limit.

```{r}
sd(xBar)
```

## c

-----------------------------------

While the standard deviation might not look familiar, the variance could:

```{r}
sd(xBar)^2
```

It looks to be about 1/10th of the true variance of a die (~2.9167) - so we can expect the standard deviation to be similar to sqrt(var/10):

```{r}
sqrt((sum(((1:6)^2)/6) - 3.5^2)/10)

sd(xBar)

```

# Problem 20

-----------------------------------

## a

-----------------------------------

```{r}
xBar <- apply(matrix(runif(1000*10), 1000), 1, mean)
```

```{r}
hist(xBar, breaks = 100)
```
## b

-----------------------------------
```{r}
mean(xBar)
```

As before, the mean of these means approaches the mean of the distribution which it was sampled (the uniform distribution has a mean of 0.5). Again this is because of the central limit theorem.

## c

-----------------------------------

```{r}
sd(xBar)
```
Similarly, the reltionship is easier to make if we take the square and compare it to the variance of the uniform distribution:

```{r}
sd(xBar)^2
sd(runif(1000*10))^2
```
Again we note that the variance of this sampling distribution is close to $Var(X)/n$ where $Var(X)$ is the variance of the distribution from which samples are taken, and $n$ is the number of 'groups' that are being aggregated into means.
