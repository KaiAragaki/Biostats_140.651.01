---
title: "BST 140.651 Problem Set 1"
author: "Kai Aragaki"
date: "9/10/2020"
output: 
        tufte::tufte_html: default
        tufte::tufte_handout: default
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "output", output_format = "all") })

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Problem 1. Show the following

## $a.\space P(\emptyset) = 0.$

-----------------------------------

$\Omega = \emptyset \cup \Omega$  
$P(\Omega)=P(\emptyset\cup\Omega)$

```{marginfigure}
Because $\emptyset\cap\Omega=\emptyset$ (ie the two sets are non-overlapping)
```
$P(\emptyset\cup\Omega)=P(\emptyset)+P(\Omega)$

So,

$P(\Omega)=P(\emptyset)+P(\Omega)$

Since $P(\Omega)=1$,

$1=P(\emptyset)+1$

And solving for $P(\emptyset)$ we have:
```{marginfigure}
$\blacksquare$
```
$P(\emptyset)=0$

## b. $P(E) = 1 - P(E^c).$

---------------------------------

$1 = P(\Omega)$

$1 = P(E\cup E^c)$

```{marginfigure}
Because $E\cap E^c = \emptyset$
```

$1 = P(E) + P(E^c)$

Solving for $P(E)$,

$P(E) = 1 - P(E^c)$
```{marginfigure}
$\blacksquare$
```

## c. $\mathrm{if}\space A\subset B, \mathrm{then}\space P(A)\le P(B)$

---------------------------------

If $A\subset B$,

$B = A \cup(A^c\cap B)$

$P(B)=P(A\cup (A^c\cap B))$

```{marginfigure}
Because $A\cap (A^c\cap B) = \emptyset$
```

$P(B)=P(A) + P(A^c\cap B))$

$P(B)-P(A)=P(A^c\cap B)$

We know that 

$P(A^c \cap B) \ge 0$

because

$0 \le P(E) \le 1$

therefore,

$P(B) - P(A) \ge 0$

Solving for $P(B)$ we arrive at

$P(B) \ge P(A)$
```{marginfigure}
$\blacksquare$
```

## d. $P(A\cup B) = P(A) + P(B) - P(A\cap B)$

---------------------------------

$A = A\cap\Omega = A\cap(B\cup B^c) = (A\cap B)\cup(A\cap B^c)$

taking the probability of both sides,

$P(A) = P((A\cap B)\cup(A\cap B^c))$

Because $(A\cap B)\cap(A\cap B^c)=\emptyset$,

$P(A) = P(A\cap B)+P(A\cap B^c)$

By the same logic, 

$P(B) = P(A\cap B)+P(B\cap A^c)$

We can substitute these into the first equation, giving

$P(A\cap B)+P(A\cap B^c) + P(A\cap B)+P(B\cap A^c) - P(A\cap B)$

Simplifying, 

$P(A\cup B) = P(A\cap B^c) + P(A\cap B)+P(B\cap A^c)$

Since we know that the sum of probabilities of two disjoint sets is equal to probability of their union, and we know
$(A\cap B) \cap (A \cap B^c) = \emptyset$

so this is equivalent to

$P((A\cap B^c)\cup(A\cap B)) + P(B\cap A^c)$

and by the distributive property,

$P(A\cap (B\cup B^c))+P(B\cap A^c)$

Which simplifies to

$P(A) + P(B\cap A^c)$

We know that 

$A \cap (B\cap A^c) = \emptyset$ 

because 

$A \cap A^c = \emptyset$ 

and 

$(B\cap A^c) \subset A^c$. 

So, 

$P(A \cup (B\cap A^c))$

which again by the distributive property,

$P((A \cup B)) \cap(A\cup A^c))$

and,

$P((A\cup B))\cap\Omega) = P(A \cup B)$
```{marginfigure}
$\blacksquare$
```

## e. $P(A\cup B) = 1-P(A^c\cap B^c)$

---------------------------------

$1 = P(\Omega) = P((A \cup B) \cup (A\cup B)^c)$

Given the sets are disjoint,

$P((A \cup B) \cup (A\cup B)^c) = P(A \cup B) + P(A\cup B)^c$

By Demorgan's law,

$P(A \cup B) + P(A\cup B)^c = P(A \cup B) + P(A^c\cap B^c)$

Substituting this into the statement we intend to prove,

$P(A\cup B) = P(A\cup B) + P(A^c\cap B^c) - P(A^c\cap B^c)$


$P(A\cup B) = P(A\cup B)$
```{marginfigure}
$\blacksquare$
```

## f. $P(A\cap B^c) = P(A)-P(A\cap B)$

---------------------------------

Rearranging the above, we note

$P(A\cap B^c) + P(A\cap B) = P(A)$

Since $(A\cap B^c) \cap (A\cap B) = \emptyset$,

$P((A\cap B^c)\cup (A\cap B)) = P(A)$

Which by the distributive property is equal to

$P(A\cap (B^c\cup B))) = P(A \cap \Omega) = P(A)$
```{marginfigure}
$\blacksquare$
```

## g. $P(\bigcup\limits^n_{i=1}E_i)\le \sum\limits_{i=1}^nP(E_i)$

---------------------------------

Proof by induction, starting with the case $n = 2$.

We know that

$P(E_1\cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$
```{marginfigure}
see proof d
```
And since
$P(E_1) + P(E_2) = \sum\limits^2_{i=1}P(E_i)$

by substitution we get

$P(E_1\cup E_2) = \sum\limits^2_{i=1}P(E_i) - P(E_1\cap E_2)$

Since the probability of any even must be greater than or equal to zero, the right hand side of the equation (and by definition the left) *must* be less than or equal to $\sum\limits^2_{i=1}P(E_i)$ itself! So,

$P(E_1\cup E_2) \le \sum\limits^2_{i=1}P(E_i)$

Since we have proven our statement for n = 2, let us assume it true for n-1:

$P(\bigcup\limits^{n-1}_{i=1}E_i) \le \sum\limits^{n-1}_{i=1}P(E_i)$

Then for n,

$P(\bigcup\limits^{n}_{i=1}E_i) = P(E_n)\cup P(\bigcup\limits^{n-1}_{i=1}E_i)$

We notice that this is structurally identical to that which we proved above, where $P(E_1) = P(E_n)$ and $P(E_2) = P(\bigcup\limits^{n-1}_{i=1}E_i)$. Following the same proof we find

$P(\bigcup\limits^n_{i=1}E_i)\le \sum\limits^n_{i=1}P(E_i))$
```{marginfigure}
$\blacksquare$
```

## h. $P(\bigcup\limits^n_{i=1}E_i)\ge max_i\space P(E_i)$

---------------------------------

For proof by induction, begin with n = 2:

$P(E_1\cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$
```{marginfigure}
proof d
```

If we suppose $P(E_1) = max_iP(E_i)$, then this is

$P(E_1) + P(E_2) - P(E_1\cap E_2) \ge P(E_1)$

Which we can simplify to

$P(E_2) \ge P(E_1\cap E_2)$

which we know to be true
```{marginfigure}
proof c
```

We now consider the case for n-1 to be true:

$P(\bigcup\limits^{n-1}_{i=1}E_i) \ge max_i\space P(E_i)$

and evaluate the truth of n:

$P(E_n)\cup P(\bigcup\limits^{n-1}_{i=1}E_i) \ge max_i\space P(E_i)$

Like proof g, we can state that

$P(E_n)$ is analogous to $P(E_1)$

and

$P(\bigcup\limits^{n-1}_{i=1}E_i)$ is analgous to $P(E_2)$, 

at which point this is a statement we have already proved
```{marginfigure}
$\blacksquare$
```

# Problem 2

Cryptosporidium is a pathogen that can cause gastrointestinal illness with diarrhea; infections can lead to death in individuals with a weakened immune system. During a recent outbreak of cryptosporidiosis in 21% of two parent families at least one of the parents has contracted the disease. In 9% of the families the father has contracted cryptosporidiosis while in 5% of the families both the mother and father have contracted cryptosporidiosis.

## a. What event does the probability one minus the probability that both have contracted cryptosporidiosis represent?

------------------------------------------------------------------

**The probability that one or fewer parents in a two parent family contracted the disease**

## b. What’s the probability that either the mother or the father has contracted cryptosporidiosis?

------------------------------------------------------------------

21%, if by this we include the possibility of the mother AND father having contracted cryptosporidiosis
If we do not include the possibility of the mother AND father, it is $P(father\cup mother)-P(father\cap mother)$ which is 21%-5%, or 

**16%**

## c. What’s the probability that the mother has contracted cryptosporidiosis but the father has not?

------------------------------------------------------------------

We can solve for $P(mother)$ first with knowledge that

$P(father\cup mother)= P(father) + P(mother) - P(father\cap mother)$

Substituting in our given values,

$21\% = 9\% + P(mother) - 5\%$

which solving for $P(mother)$ gives 17%. Subtracting the intersect gives

**12%**

## d. What’s the probability that the mother has contracted cryptosporidiosis?

------------------------------------------------------------------

This was calculated above to be

**17%**

## e. What’s the probability that neither the mother nor the father has contracted cryptosporidiosis?

------------------------------------------------------------------

100% - 21% = **79%**

## f. What’s the probability that the mother has contracted cryptosporidiosis but the father has not?

**Identical to problem c**


# Problem 3

------------------------------------------------------------------

A valid pmf follows two rules: $0 \ge p(h(x)) \ge 1$ for all values and $\sum\limits_1^Ip(x) = 1$.

To the first point, all input values are positive, so all probabilities are positive by this function. Additionally, $h(x)\le\sum\limits^I_{i = 1}h(i)$ since $\sum\limits^I_{i = 1}h(i)$ is a sum of non-zero values (given) from $h(x)$, so 

$\sum\limits^I_{i = 1}h(i) \ge h(x) \implies \frac{h(x)}{\sum^I_{i = 1}h(i)} \le 1 \implies p(x) \le 1$


To the second point, 

$\sum\limits_1^Ip(x) = \frac{\sum_1^I(h(x)}{\sum_1^Ih(x))} = \frac{\sum_1^Ih(x)}{\sum_1^Ih(x)} = 1$
```{marginfigure}
$\blacksquare$
```

# Problem 4

------------------------------------------------------------------

A valid pdf follows two rules: $0 \ge f(h(x)) \ge 1$ across the function, and $\int f(x)dx = 1$ for all values.

Since we know that $h > 0$, all possible values of the function $f(x)$ can also only be positive. 

We can show by similar logic above (albeit on an infinitesimal level) that $h(x) \le c$ since $c$ is a sum of non-zero (given) values from $h(x)$. Therefore $h(x)/c \le 1$.

To the second point, 

$\int\limits_{-\infty}^\infty f(x)dx = \int\limits_{-\infty}^\infty h(x)dx/c$

but we were told

$c = \int\limits_{-\infty}^\infty h(x)dx$

so 

$\int\limits_{-\infty}^\infty f(x)dx = \frac{c}{c} = 1$
```{marginfigure}
$\blacksquare$
```

# Problem 5

------------------------------------------------------------------

## a.

------------------------------------------------------------------

```{r}
dat <- tibble(x = seq(-1, 2, by = 0.01),
              y = dunif(x))
ggplot(dat, aes(x = x, y = y)) + geom_line()
```
Since we know that the area under the curve of a PDF must be equal to 1, and the distribution goes from 0 to 1, k must equal 1.

## b

------------------------------------------------------------------

Mathematically we can represent this as 

$\int\limits^.7_.1f(x)dx = .6$

In the context of the problem, we can think about this as "what is the probability that there exists an individual within this population that has proportion of their skin covered in freckles somewhere between 0.1 and 0.7?

## c

------------------------------------------------------------------

```{r}
punif(0.7) - punif(0.1)
```

We can see that for this particular distribution, the probability that $a < X < b$ given $0<a<b<1$ is $b-a$

## d

------------------------------------------------------------------

pdf

## e 

------------------------------------------------------------------

The median (or 50%) is the probability (or probability density) at the point where half the values are greater than the given value, and half are lower. In the context of the uniform distribution, the median probability is 0.5. In the context of our problem, this states that half the people in the population have greater than half their skin covered in freckles, while the other half of the population have less than half their skin covered in freckles.


## f 

------------------------------------------------------------------

Here the 95% is 0.95 (lower tail) - that is, in the context of this problem, 95% of the population will have a smaller proportion of their skin befreckled

## g 

------------------------------------------------------------------

This seems unlikely to me. In the real world there are given subpopulations that tend to have similar amounts of clusters (tending more towards a collection of gaussians vs a completely smooth uniform distribution). Intuitively it also seems strange that one could have 0 freckles just as probably as one could have 1 freckle - it seems more likely that if one has one freckle, there are probably more.



# Problem 6

------------------------------------------------------------------

## a

------------------------------------------------------------------

A random variable $U$ is passed to a function $F^{-1}(U)$ which we will define as a new random variable, $X$.

Our intent is to find that $CDF(X) = F$

We define a CDF as the probability of obtaining a value of or less than that value. That is,

$CDF(X) = Pr(X \le x)$

or, using our definition of $X$ above,

$Pr(X \le x) = Pr(F^{-1}(U) \le x)$

We were told that $F$ was strictly increasing, and is therefore invertable. We take $F$ of both sides of the inequality to get

$Pr(F^{-1}(U) \le x) = Pr(F(F^{-1}(U)\le F(x))$

and use the knowledge that for invertable functions,

$F(F^{-1}(Y)) = Y$

so

$Pr(F(F^{-1}(U)\le F(x))= Pr(U \le F(x))$

The CDF can also be rewritten as

$\int\limits_{lower}^y pdf(Y)dy$

Where $Y$ is a random variable. In this instance we know that the pdf of $U$ is a uniform distribution, which for $[0,1]$ is $1$. Additionally, in this instance our upper bound is $F(x)$, so

$\int\limits_0^{F(x)} 1dx = F(x)$
```{marginfigure}
$\blacksquare$
```

## b

------------------------------------------------------------------

Since we know that values of $F^{-1}(U)$ have a cdf equal to $F$, we can generate $n$ simulated points by doing

```{r}
set.seed(123)
n <- 100
points <- runif(100, min = 0, max = 1)
```

then we can feed the `points` argument into our function $F$ to get a set of 100 points with a cdf approximating $F$.

## c

------------------------------------------------------------------

```{r}
set.seed(123)
n <- 100
points <- runif(100, min = 0, max = 1)
out <- dnorm(points, mean = .5, sd = 0.25)
plot(points, out)
```

# Problem 7

------------------------------------------------------------------

## a

------------------------------------------------------------------

A valid pdf has an integral equal to 1 and for for each value of x, $0 \le g(x) \le 1$.

Since we know that $0 \le \pi \le1$ and that $f_1(x)$ and $f_2(x)$ are continuous densities, we can discern that $g(x) \ge 0$ for all values of x. Additionally we can show that $g(x) \le 1$ if we look at the maximum values possible for $g(x)$, namely if both $f_1$ and $f_2$ are 1, whice evaluates to:
$g(x)$ can be expanded to

$g(x) = \pi + 1-\pi = 1$

The integral of a valid continous density is equal to 1, so

$\int g(x)dx = \pi\int f_1(x)dx + (1-\pi)\int f_2(x)dx = \pi+1-\pi=1$
```{marginfigure}
$\blacksquare$
```

## b

------------------------------------------------------------------

The distribution function is just the integral of the density. Therefore

$G(x) = \pi F_1(x) + (1-\pi)F_2(x)$

Where $G(x)$ is thee distribution function associated with $g$


## c

------------------------------------------------------------------

The survival function is simply 1-cdf, so

$1 - G(x) =1 - (\pi F_1(x) + (1-\pi)F_2(x))$

Which can be simplified to 

$1 - G(x) =1 - \pi F_1(x) - F_2(x) + \pi F_2(x)$

And by substituting $1-F$ for $S$,

$S_g(x) =(1+\pi)S_2(x) + (2-\pi)S_1(x)$


# Problem 8

------------------------------------------------------------------

## a

------------------------------------------------------------------

$e^{-x}$ is always positive, so for any given value this function is > 0. 

We can integrate to ensure that the area under the curve = 1. By substitution where $a = 1 + e^{-x}$, we see that the integral evaluates to

$(1+e^{-x})^{-1}|^\infty_{-\infty} = 1 - 0 = 1$
```{marginfigure}
$\blacksquare$
```
## b

------------------------------------------------------------------

The cdf is simply the integral of the density from its lower bound to some value x:

$(1+e^{-x})^{-1}|^x_{-\infty}$

$(1+e^{-x})^{-1}$

## c

------------------------------------------------------------------

Plugging in 0 into the distribution equation yields 1, which in the context of this problem may mean 'having a score of 0 of this particular risk factor or this diagnostic readout leads to a completely certain chance of cancer'.

## d

------------------------------------------------------------------

The $p^th$ quantile is defined as the point on the cdf where $F(x) = p$. We also know that the cdf is $(1+e^{-x})^{-1}$. 

So, solving for x:

$(1+e^{-x})^{-1} = p$
$1+e^{-x} = \frac{1}{p}$
$e^{-x} = $
$-x = log(\frac{(1-p)}{p})$
$x = log(\frac{(1-p)}{p}^{-1}) = log(\frac{p}{(1-p)})$
```{marginfigure}
$\blacksquare$
```


# Problem 9

------------------------------------------------------------------

## a

------------------------------------------------------------------

The probability density is just the derivative. Given $x_0$ and $\alpha$ are constant, the pdf is

$\alpha(\frac{x_0}{x})^{a-1}(x^{-2})$

or more simply

$\frac{\alpha x_0^\alpha}{x^{\alpha+1}}$

## b

------------------------------------------------------------------

```{r}

make_pareto_cdf <- function(x, x0, a) {
  if (x < x0) {
    0
  }
  else {
    1 - (x0/x)^a
  }
}

make_pareto_pdf <- function(x, x0, a) {
  if (x < x0) {
    0
  }
  else {
    a*x0^a/x^(a+1)
  }
}

pareto <- expand_grid(x = seq(0,110, 0.1), x0 = c(1, 2, 5), a = c(0.1, 1, 10)) %>% 
  mutate(y_cdf = unlist(pmap(list(x, x0, a), make_pareto_cdf)),
         y_pdf = unlist(pmap(list(x, x0, a), make_pareto_pdf))) %>% 
  pivot_longer(cols = c(y_cdf, y_pdf), names_to = "fun")

ggplot(pareto, aes(x = x, y = value, color = fun)) + geom_line() + facet_grid(a~x0) + coord_cartesian(ylim = c(0, 1), xlim = c(0, 10))

```
The scale parameter seems to shift the beginning of the function (likely relating to the 'base' number of years a product is expected to last before failing) while the shape parameter seems inversely related to the sharpness of the pdf peak - likely relating to the onset of failure after the given 'due date'.

## c

------------------------------------------------------------------

We know that by feeding samples from a uniform distribution into the inverse of the CDF for the Pareto distribution we can get numbers that are similarly distributed as to those sampled from a random variable with a distribution resembling the Pareto distribution.

$x = 1-(\frac{x_0}{F^{-1}(x)})^\alpha$
$(\frac{x_0}{F^{-1}(x)})^\alpha= 1-x$
$log(\frac{x_0}{F^{-1}(x)})= \frac{log(1-x)}\alpha$
$log(x_0) - log(F^{-1}(x))= \frac{log(1-x)}{\alpha}$
$log(F^{-1}(x))= log(x_0) - log(1-x)^{1/\alpha}$
$F^{-1}(x)= \frac{x_0}{(1-x)}^{1/\alpha}$

```{r}
x <- runif(10000)
y <- function(x, x0, a){
  (x0/(1-x))^(1/a)
}

pareto <- tibble(x, y = y(x, 1, 2))


ggplot(pareto, aes(x = y)) + geom_histogram(binwidth = .1) + coord_cartesian(xlim = c(0, 10))

```

## d

------------------------------------------------------------------

Survival = 1-cdf

$S(x) = \Big\{^{(x_0/x)^\alpha}_{1}$

For values $x = 10$ years, $\alpha = 1$, and $x_0 = 2$,

$S(10) = \frac{2}{10}^1 = 1/5$

This could be interpreted as the likelihood that a given part will still be functioning at 10 years given an expected lifespan of 2 years

## e

------------------------------------------------------------------

We previously found the inverse of the cdf, which is the quantile function:

$F^{-1}(x)= (\frac{x_0}{1-x})^{1/\alpha}$ 

With an input of $p = .8$, this function would return the number of years that it would be expected that 80% of units have failed.


# Problem 10

------------------------------------------------------------------

## a

------------------------------------------------------------------

If this is to be a valid density, the integral from 0 to 1 should be 1. That is,

$\int\limits^1_0cx^kdx=1$
$c(\frac{x^{k+1}}{k+1}\big|^1_0)=1$
$c/(k+1)=1$
$c=k+1$

## b

------------------------------------------------------------------

The cdf is just the integral from the lower bound to x, so 

$F(x) = \int\limits^x_0cx^kdx$
$F(x) = \frac{cx^{k+1}}{k+1}|^x_0$
$F(x) = \frac{cx^{k+1}}{(k+1)}$

and since we know $c=k+1$, 

$F(x) = x^{k+1}$


## c

------------------------------------------------------------------

The quantile formula is the inverse cdf, so

$x= F^{-1}(x)^{k+1}$

$log(x)= (k+1)log(F^{-1}(x))$

$log(F^{-1}(x)) = \frac{log(x)}{(k+1)}$

$log(F^{-1}(x)) = log(x^{\frac{1}{(k+1)}})$

$F^{-1}(x) = x^{\frac{1}{(k+1)}}$


## d

------------------------------------------------------------------

This is simply the cdf where the lower bound is $a$ and the upper bound is $b$, so

$F(x) = \frac{cx^{k+1}}{k+1}\big|^b_a$

$F(x) = x^{k+1}\big|^b_a$

$F(x) = b^{k+1}-a^{k+1}$


# Problem 11

------------------------------------------------------------------

## a

------------------------------------------------------------------

To make this a valid density, the following must be true:

$\int\limits_0^\infty f(x)dx = 1$

$\int\limits_0^\infty ce^{-x/2.5}dx = 1$

$-2.5c(e^{-x/2.5}\big|^\infty_0)=1$

$-2.5c(0-1)=1$

$c = \frac{1}{2.5}$


## b

------------------------------------------------------------------

This is the integral of the pdf from the lower bound (0) to x:

$\int\limits_0^x f(x)dx$

$\int\limits_0^x \frac{e^{-x/2.5}}{2.5}dx$

$-(e^{-x/2.5}-1)$

$F(x) = 1-e^{-x/2.5}$

## c

------------------------------------------------------------------

The survival function is  1-cdf or

$S(x) = 1-(1-e^{-x/2.5})$

$S(x) = e^{-x/2.5}$


## d

------------------------------------------------------------------

$S(11) = e^{-11/2.5} \approx 0.012$


## e

------------------------------------------------------------------

This involves finding the quantile function, which is the inverse of the cdf:

$x = 1-e^{-F^{-1}(x)/2.5}$

$1-x = e^{-F^{-1}(x)/2.5}$

$log(1-x) = \frac{-F^{-1}(x)}{2.5}$

$F^{-1}(x) = -2.5log(1-x)$

And then find the value where $x = 0.5$

$F^{-1}(0.5) = -2.5log(.5) \approx 1.73$


# Problem 12

## a

------------------------------------------------------------------

To show that it is a valid density, we must show

$\int\limits^\infty_0\frac{x^{\alpha-1}e^{-x}dx}{\Gamma(\alpha)}$

We note that the integral is the same as

$\frac{\Gamma(\alpha)}{\Gamma(\alpha)} = 1$

We can also show that the density is positive for all values of $x >0$ by noting that $x^{\alpha-1}$ is always greater than 0 for all values of $x > 0$, $e^{-x}$ approaches 0 and thus is always positive, and as $\Gamma(\alpha) = (\alpha - 1)!$, for $\alpha > 1$ we know it to be positive as well. This, where $x > 0$ and $\alpha > 1$, this function is positive for all values.

## b

------------------------------------------------------------------

We know that the integral of the pdf from the lower bound (here $0$) to $x$ is the cdf, and the survival function is 1 - cdf, so

$S(x) = 1 - \int\limits^x_0\frac{x^{\alpha-1}e^{-x}dx}{\Gamma(\alpha)}$

$S(x) = 1 - \frac{\Gamma(\alpha, x)}{\Gamma(\alpha)}$

## c

------------------------------------------------------------------

For the gamma density to be a valid density, the function must be greater than 0 for all values of x within the bounds (here $0$ to $\infty$). We note that like in $a)$ each individual component is positive so long as $\alpha > 1, \beta > 0$.

Additionally, the integral from lower to upper bound must equal 1, which we can show:

$\int\limits^\infty_0\frac{x^{\alpha-1}e^{-x/\beta}dx}{\beta^\alpha\Gamma(\alpha)}$

With substitution, where 

$u = x/\beta$
and
$du = dx/\beta$

we find that our equation can be written as

$\int\limits^\infty_0\frac{u^{\alpha-1}e^{-u}du}{\Gamma(\alpha)}$

We notice that the numerator is simply $\Gamma(\alpha)$ so,

$\frac{\Gamma(\alpha)}{\Gamma(\alpha)} = 1$

## d 

------------------------------------------------------------------

```{r}
tib <- expand_grid(a = c(1:5), b = c(1:5), x = 0:100) %>% 
  mutate(y = (x^(a-1)*exp(-x/b))/(b^a*factorial(a-1)))

ggplot(tib, aes(x = x, y = y)) + geom_line() + facet_grid(a~b)
```

# Problem 13

------------------------------------------------------------------

## a

------------------------------------------------------------------

We take the integral from lower to upper bound and see if:

$\int\limits^\infty_0\frac{\gamma x^{\gamma-1}e^{-x^\gamma/\beta}}{\beta} = 1$

By substitution, if

$u = x^\gamma/\beta$

and

$du = \frac{\gamma x^{\gamma - 1}}{\beta dx}$

and we notice that

$\frac{du\beta}{\gamma} = x^{\gamma-1}dx$

we can write the original equation as

$\int\limits^\infty_0 e^{-u}du = -e^{-u}\Big|^\infty_0 = 0--1 = 1$

additionally we note that neither $x^c$ nor $e^-x$ can become negative with values of $x > 0$.

## b

------------------------------------------------------------------

We found the integral of the pdf to be  

$-e^{-u} = -e^{-x^\gamma/\beta}$

And the cdf is defined by the integral from the lower bound to some value $x$, so

$F(x) = -e^{-x^\gamma/\beta} + 1$

where $F(x)$ is the cdf.

We also know the surival function to be $1 - F(x)$, so 

$S(x) = e^{-x^\gamma/\beta}$

## c

------------------------------------------------------------------

We first must find the quantile function, which is the inverse cumulative distribution function:

$x = -e^{-F^{-1}(x)^\gamma/\beta} + 1$

$1-x = e^-F^{-1}(x)^\gamma/\beta$

$-log(1-x) = F^{-1}(x)^\gamma/\beta$

$-\beta log(1-x)=F^{-1}(x)^\gamma$

$F^{-1}(x) = \frac{log(-\beta log(1-x))}{\gamma}$

The median is such that

$F^{-1}(0.5) = \frac{log(-\beta log(1-0.5))}{\gamma} = \frac{log(-\beta log(.5))}{\gamma}$

## d

------------------------------------------------------------------

```{r}
tib <- expand_grid(x = seq(1, 10, by = 0.1), g = c(0.5, 1, 2), b = c(0.5, 1, 2)) %>% 
  mutate(y = (g/b)*x^(g-1)*exp((-x^g)/b))

ggplot(tib, aes(x = x, y = y)) + geom_line() + facet_grid(g~b)
```


# Problem 14

------------------------------------------------------------------

## a

------------------------------------------------------------------

First we show that the integral from lower to upper bound = 1:

$\frac{1}{B(\alpha, \beta)}\int\limits^1_0x^{\alpha-1}(1-x)^{\beta-1}$

$\frac{B(\alpha, \beta)}{B(\alpha, \beta)} = 1$

Additionally we know that $x^c$ is always positive as long as $x > 0$ and $(1-x)^c$ is positive so long as $x \le 1$. These conditions both fall within our bounds.

## b

------------------------------------------------------------------

The uniform density is 1 from 0 to 1. For the uniform density to be a special case of the Beta density, there must be some value of $\alpha$ and $\beta$ such that $x = 1 |x \in [0, 1]$ and $\int^1_0B(\alpha,\beta)=1$

In the case of $\alpha = 1, \beta = 1$, 

$B(1,1)=\frac{\Gamma(1)\Gamma(1)}{\Gamma(2)} = 0!0!/1! = 1$

so the Beta density function simplifies to

$x^{1-1}(1-x)^{1-1}=1$

1 is 1 from 0 to 1. The integral is

$\int\limits^1_01dx = x\big|^1_0=1-0=1$

so it is also valid in this regard as well.


## c

------------------------------------------------------------------

```{r}
tib <- expand_grid(x = seq(0, 1, by = 0.01), a = c(0.5, 1, 3, 5), b = c(0.5, 1, 3, 5)) %>% 
  mutate(beta = factorial(a-1)*factorial(b-1)/factorial(a + b - 2),
         y = (1/beta)*x^(a-1)*(1-x)^(b-1))

ggplot(tib, aes(x = x, y = y)) + geom_line() + facet_grid(a~b) + coord_cartesian(ylim = c(0, 2))
```

# Problem 15 

------------------------------------------------------------------

## a

------------------------------------------------------------------

Suppose 

$\sum\limits^\infty_{x=0}\frac{(e^{-\lambda}\lambda^x)}{x!} = 1$

this can be written as

$\frac{(e^{-\lambda}\lambda^0)}{0!} + \frac{(e^{-\lambda}\lambda^1)}{1!} + ...+ \frac{(e^{-\lambda}\lambda^n)}{n!}  = 1$

which we can factor the constant out from:

$e^{-\lambda}(\frac{\lambda^0}{0!} + \frac{\lambda^1}{1!} + ...) = 1$

and by taking the natural log of both sizes

$-\lambda+ log(\frac{\lambda^0}{0!} + \frac{\lambda^1}{1!} + ...) = 0$

which is 

$log(\frac{\lambda^0}{0!} + \frac{\lambda^1}{1!} + ...) = \lambda$

$\frac{\lambda^0}{0!} + \frac{\lambda^1}{1!} + ... = e^\lambda$

And we can look up the Taylor series of $e^x$ to find it is of the form

$e^x = \frac{x^0}{0!} + \frac{x^1}{1!} + \frac{x^2}{2!}+...$

which matches the left hand side when $x = \lambda$, so

$e^\lambda= e^\lambda$
```{marginfigure}
$\blacksquare$
```

# Problem 16

------------------------------------------------------------------

## a

------------------------------------------------------------------

To be a valid pmf, the sum of all values must total to 1 and no value can be below 0.

For the second point, if $0 \le p \le 1$, then $p$ will always remain positive and $1-p$ will always remain positive (and by extension $(1-p)^x$ will remain positive so long as $x \ge 0$). Therefore their product will always be greater than 0.

To find if the sum converges to 1, we define $s$ as

$p(1-p)^0 + p(1-p)^1 + p(1-p)^2 + ...$

And therefore $s(1-p)$ is

$p(1-p)^1 + p(1-p)^2 + p(1-p)^3 + ...$

So 

$s - s(1-p) = p$
$s(1-1+p) = p$
$sp = p$
$s = 1$
```{marginfigure}
$\blacksquare$
```

## b

------------------------------------------------------------------

Given the cdf 

$\sum\limits_{x = 1}^np(1-p)^{x-1}$

Which we can write out as

$p((1-p)^0 +...+ (1-p)^{n-1})$

If we define 

$s = (1-p)^0 +...+ (1-p)^{n-1}$

then

$s(1-p) = (1-p)^1 +...+ (1-p)^{n}$

so that

$s - s(1-p) = (1-p)^0 - (1-p)^n$

Simplified,

$sp = 1 - (1-p)^n$

Returning to our original equation, 

$\sum\limits_{x = 1}^np(1-p)^{x-1} = ps = 1 - (1-p)^n$

So then the survival function is given by 1 - cdf, or

$S(x) =1- 1 + (1-p)^x = (1-p)^x$
```{marginfigure}
$\blacksquare$
```
