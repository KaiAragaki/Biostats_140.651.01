---
title: "BST 140.651 Problem Set 1"
author: "Kai Aragaki"
date: "9/10/2020"
output: 
        tufte::tufte_html: default
        tufte::tufte_handout: default
header-includes:
   - \setlength\parindent{0pt}
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_dir = "output", output_format = "all") })

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

# Problem 1. Show the following

## $a.\space P(\emptyset) = 0.$

-----------------------------------

$\Omega = \emptyset \cup \Omega$  
$P(\Omega)=P(\emptyset\cup\Omega)$

```{marginfigure}
Because $\emptyset\cap\Omega=\emptyset$ (ie the two sets are non-overlapping)
```
$P(\emptyset\cup\Omega)=P(\emptyset)+P(\Omega)$

So,

$P(\Omega)=P(\emptyset)+P(\Omega)$

Since $P(\Omega)=1$,

$1=P(\emptyset)+1$

And solving for $P(\emptyset)$ we have:
```{marginfigure}
$\blacksquare$
```
$P(\emptyset)=0$

## b. $P(E) = 1 - P(E^c).$

---------------------------------

$1 = P(\Omega)$

$1 = P(E\cup E^c)$

```{marginfigure}
Because $E\cap E^c = \emptyset$
```

$1 = P(E) + P(E^c)$

Solving for $P(E)$,

$P(E) = 1 - P(E^c)$
```{marginfigure}
$\blacksquare$
```

## c. $\mathrm{if}\space A\subset B, \mathrm{then}\space P(A)\le P(B)$

---------------------------------

If $A\subset B$,

$B = A \cup(A^c\cap B)$

$P(B)=P(A\cup (A^c\cap B))$

```{marginfigure}
Because $A\cap (A^c\cap B) = \emptyset$
```

$P(B)=P(A) + P(A^c\cap B))$

$P(B)-P(A)=P(A^c\cap B)$

We know that 

$P(A^c \cap B) \ge 0$

because

$0 \le P(E) \le 1$

therefore,

$P(B) - P(A) \ge 0$

Solving for $P(B)$ we arrive at

$P(B) \ge P(A)$
```{marginfigure}
$\blacksquare$
```

## d. $P(A\cup B) = P(A) + P(B) - P(A\cap B)$

---------------------------------

$A = A\cap\Omega = A\cap(B\cup B^c) = (A\cap B)\cup(A\cap B^c)$

taking the probability of both sides,

$P(A) = P((A\cap B)\cup(A\cap B^c))$

Because $(A\cap B)\cap(A\cap B^c)=\emptyset$,

$P(A) = P(A\cap B)+P(A\cap B^c)$

By the same logic, 

$P(B) = P(A\cap B)+P(B\cap A^c)$

We can substitute these into the first equation, giving

$P(A\cap B)+P(A\cap B^c) + P(A\cap B)+P(B\cap A^c) - P(A\cap B)$

Simplifying, 

$P(A\cup B) = P(A\cap B^c) + P(A\cap B)+P(B\cap A^c)$

Since we know that the sum of probabilities of two disjoint sets is equal to probability of their union, and we know
$(A\cap B) \cap (A \cap B^c) = \emptyset$

so this is equivalent to

$P((A\cap B^c)\cup(A\cap B)) + P(B\cap A^c)$

and by the distributive property,

$P(A\cap (B\cup B^c))+P(B\cap A^c)$

Which simplifies to

$P(A) + P(B\cap A^c)$

We know that 

$A \cap (B\cap A^c) = \emptyset$ 

because 

$A \cap A^c = \emptyset$ 

and 

$(B\cap A^c) \subset A^c$. 

So, 

$P(A \cup (B\cap A^c))$

which again by the distributive property,

$P((A \cup B)) \cap(A\cup A^c))$

and,

$P((A\cup B))\cap\Omega) = P(A \cup B)$
```{marginfigure}
$\blacksquare$
```

## e. $P(A\cup B) = 1-P(A^c\cap B^c)$

---------------------------------

$1 = P(\Omega) = P((A \cup B) \cup (A\cup B)^c)$

Given the sets are disjoint,

$P((A \cup B) \cup (A\cup B)^c) = P(A \cup B) + P(A\cup B)^c$

By Demorgan's law,

$P(A \cup B) + P(A\cup B)^c = P(A \cup B) + P(A^c\cap B^c)$

Substituting this into the statement we intend to prove,

$P(A\cup B) = P(A\cup B) + P(A^c\cap B^c) - P(A^c\cap B^c)$


$P(A\cup B) = P(A\cup B)$
```{marginfigure}
$\blacksquare$
```

## f. $P(A\cap B^c) = P(A)-P(A\cap B)$

---------------------------------

Rearranging the above, we note

$P(A\cap B^c) + P(A\cap B) = P(A)$

Since $(A\cap B^c) \cap (A\cap B) = \emptyset$,

$P((A\cap B^c)\cup (A\cap B)) = P(A)$

Which by the distributive property is equal to

$P(A\cap (B^c\cup B))) = P(A \cap \Omega) = P(A)$
```{marginfigure}
$\blacksquare$
```

## g. $P(\cup^n_{i=1}E_i)\le \Sigma^n_{i=1}P(E_i))$

---------------------------------

Proof by induction, starting with the case $n = 2$.

We know that

$P(E_1\cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$
```{marginfigure}
see proof d
```
And since
$P(E_1) + P(E_2) = \Sigma^2_{i=1}P(E_i)$

by substitution we get

$P(E_1\cup E_2) = \Sigma^2_{i=1}P(E_i) - P(E_1\cap E_2)$

Since the probability of any even must be greater than or equal to zero, the right hand side of the equation (and by definition the left) *must* be less than or equal to $\Sigma^2_{i=1}P(E_i)$ itself! So,

$P(E_1\cup E_2) \le \Sigma^2_{i=1}P(E_i)$

Since we have proven our statement for n = 2, let us assume it true for n-1:

$P(\cup^{n-1}_{i=1}E_i) \le \Sigma^{n-1}_{i=1}P(E_i)$

Then for n,

$P(\cup^{n}_{i=1}E_i) = P(E_n)\cup P(\cup^{n-1}_{i=1}E_i)$

We notice that this is structurally identical to that which we proved above, where $P(E_1) = P(E_n)$ and $P(E_2) = P(\cup^{n-1}_{i=1}E_i)$. Following the same proof we find

$P(\cup^n_{i=1}E_i)\le \Sigma^n_{i=1}P(E_i))$
```{marginfigure}
$\blacksquare$
```

## h. $P(\cup^n_{i=1}E_i)\ge max_i\space P(E_i)$

---------------------------------

For proof by induction, begin with n = 2:

$P(E_1\cup E_2) = P(E_1) + P(E_2) - P(E_1\cap E_2)$
```{marginfigure}
proof d
```

If we suppose $P(E_1) = max_iP(E_i)$, then this is

$P(E_1) + P(E_2) - P(E_1\cap E_2) \ge P(E_1)$

Which we can simplify to

$P(E_2) \ge P(E_1\cap E_2)$

which we know to be true
```{marginfigure}
proof c
```

We now consider the case for n-1 to be true:

$P(\cup^{n-1}_{i=1}E_i) \ge max_i\space P(E_i)$

and evaluate the truth of n:

$P(E_n)\cup P(\cup^{n-1}_{i=1}E_i) \ge max_i\space P(E_i)$

Like proof g, we can state that

$P(E_n)$ is analogous to $P(E_1)$

and

$P(\cup^{n-1}_{i=1}E_i)$ is analgous to $P(E_2)$, 

at which point this is a statement we have already proved
```{marginfigure}
$\blacksquare$
```

# Problem 2

Cryptosporidium is a pathogen that can cause gastrointestinal illness with diarrhea; infections can lead to death in individuals with a weakened immune system. During a recent outbreak of cryptosporidiosis in 21% of two parent families at least one of the parents has contracted the disease. In 9% of the families the father has contracted cryptosporidiosis while in 5% of the families both the mother and father have contracted cryptosporidiosis.

## a. What event does the probability one minus the probability that both have contracted cryptosporidiosis represent?

------------------------------------------------------------------

**The probability that one or fewer parents in a two parent family contracted the disease**

## b. What’s the probability that either the mother or the father has contracted cryptosporidiosis?

------------------------------------------------------------------

21%, if by this we include the possibility of the mother AND father having contracted cryptosporidiosis
If we do not include the possibility of the mother AND father, it is $P(father\cup mother)-P(father\cap mother)$ which is 21%-5%, or 

**16%**

## c. What’s the probability that the mother has contracted cryptosporidiosis but the father has not?

------------------------------------------------------------------

We can solve for $P(mother)$ first with knowledge that

$P(father\cup mother)= P(father) + P(mother) - P(father\cap mother)$

Substituting in our given values,

$21\% = 9\% + P(mother) - 5\%$

which solving for $P(mother)$ gives 17%. Subtracting the intersect gives

**12%**

## d. What’s the probability that the mother has contracted cryptosporidiosis?

------------------------------------------------------------------

This was calculated above to be

**17%**

## e. What’s the probability that neither the mother nor the father has contracted cryptosporidiosis?

------------------------------------------------------------------

100% - 21% = **79%**

## f. What’s the probability that the mother has contracted cryptosporidiosis but the father has not?

**Identical to problem c**


# Problem 3

------------------------------------------------------------------

A valid pmf follows two rules: $0 \ge p(h(x)) \ge 1$ for all values and $\Sigma_1^Ip(x) = 1$.

To the first point, all input values are positive, so all probabilities are positive by this function. Additionally, $h(x)\le\Sigma^I_{i = 1}h(i)$ since $\Sigma^I_{i = 1}h(i)$ is a sum of non-zero values (given) from $h(x)$, so 

$\Sigma^I_{i = 1}h(i) \ge h(x) \implies h(x)/\Sigma^I_{i = 1}h(i) \le 1 \implies p(x) \le 1$


To the second point, 

$\Sigma_1^Ip(x) = \Sigma_1^I(h(x)/ \Sigma_1^Ih(x)) = \Sigma_1^Ih(x)/ \Sigma_1^Ih(x) = 1$
```{marginfigure}
$\blacksquare$
```

# Problem 4

------------------------------------------------------------------

A valid pdf follows two rules: $0 \ge f(h(x)) \ge 1$ across the function, and $\int f(x)dx = 1$ for all values.

Since we know that $h > 0$, all possible values of the function $f(x)$ can also only be positive. 

We can show by similar logic above (albeit on an infinitesimal level) that $h(x) \le c$ since $c$ is a sum of non-zero (given) values from $h(x)$. Therefore $h(x)/c \le 1$.

To the second point, 

$\int_{-\infty}^\infty f(x)dx = \int_{-\infty}^\infty h(x)dx/c$

but we were told

$c = \int_{-\infty}^\infty h(x)dx$

so 

$\int_{-\infty}^\infty f(x)dx = c/c = 1$
```{marginfigure}
$\blacksquare$
```

# Problem 5

------------------------------------------------------------------

## a.

------------------------------------------------------------------

```{r}
dat <- tibble(x = seq(-1, 2, by = 0.01),
              y = dunif(x))
ggplot(dat, aes(x = x, y = y)) + geom_line()
```
Since we know that the area under the curve of a PDF must be equal to 1, and the distribution goes from 0 to 1, k must equal 1.

## b

------------------------------------------------------------------

Mathematically we can represent this as 

$\int^.7_.1f(x) = .6$

In the context of the problem, we can think about this as "what is the probability that there exists an individual within this population that has proportion of their skin covered in freckles somewhere between 0.1 and 0.7?

## c

------------------------------------------------------------------

```{r}
punif(0.7) - punif(0.1)
```

We can see that for this particular distribution, the probability that $a < X < b$ given $0<a<b<1$ is $b-a$

## d

------------------------------------------------------------------

pdf

## e 

------------------------------------------------------------------

The median (or 50%) is the probability (or probability density) at the point where half the values are greater than the given value, and half are lower. In the context of the uniform distribution, the median probability is 0.5. In the context of our problem, this states that half the people in the population have greater than half their skin covered in freckles, while the other half of the population have less than half their skin covered in freckles.


## f 

------------------------------------------------------------------

Here the 95% is 0.95 (lower tail) - that is, in the context of this problem, 95% of the population will have a smaller proportion of their skin befreckled

## g 

------------------------------------------------------------------

This seems unlikely to me. In the real world there are given subpopulations that tend to have similar amounts of clusters (tending more towards a collection of gaussians vs a completely smooth uniform distribution). Intuitively it also seems strange that one could have 0 freckles just as probably as one could have 1 freckle - it seems more likely that if one has one freckle, there are probably more.



# Problem 6

------------------------------------------------------------------

## a

------------------------------------------------------------------

A random variable $U$ is passed to a function $F^{-1}(U)$ which we will define as a new random variable, $X$.

Our intent is to find that $CDF(X) = F$

We define a CDF as the probability of obtaining a value of or less than that value. That is,

$CDF(X) = Pr(X \le x)$

or, using our definition of $X$ above,

$Pr(X \le x) = Pr(F^{-1}(U) \le x)$

We were told that $F$ was strictly increasing, and is therefore invertable. We take $F$ of both sides of the inequality to get

$Pr(F^{-1}(U) \le x) = Pr(F(F^{-1}(U)\le F(x))$

and use the knowledge that for invertable functions,

$F(F^{-1}(Y)) = Y$

so

$Pr(F(F^{-1}(U)\le F(x))= Pr(U \le F(x))$

The CDF can also be rewritten as

$\int_{lower\space bound}^y pdf(Y)dy$

Where $Y$ is a random variable. In this instance we know that the pdf of $U$ is a uniform distribution, which for $[0,1]$ is $1$. Additionally, in this instance our upper bound is $F(x)$, so

$\int_0^{F(x)} 1dx = F(x)$
```{marginfigure}
$\blacksquare$
```

## b

------------------------------------------------------------------

Since we know that values of $F^{-1}(U)$ have a cdf equal to $F$, we can generate $n$ simulated points by doing

```{r}
set.seed(123)
n <- 100
points <- runif(100, min = 0, max = 1)
```

then we can feed the `points` argument into our function $F$ to get a set of 100 points with a cdf approximating $F$.

## c

------------------------------------------------------------------

```{r}
set.seed(123)
n <- 100
points <- runif(100, min = 0, max = 1)
out <- dnorm(points, mean = .5, sd = 0.25)
plot(points, out)
```

# Problem 7

------------------------------------------------------------------

## a

------------------------------------------------------------------

A valid pdf has an integral equal to 1 and for for each value of x, $0 \le g(x) \le 1$.

Since we know that $0 \le \pi \le1$ and that $f_1(x)$ and $f_2(x)$ are continuous densities, we can discern that $g(x) \ge 0$ for all values of x. Additionally we can show that $g(x) \le 1$ if we look at the maximum values possible for $g(x)$, namely if both $f_1$ and $f_2$ are 1, whice evaluates to:
$g(x)$ can be expanded to

$g(x) = \pi + 1-\pi = 1$

The integral of a valid continous density is equal to 1, so

$\int g(x) = \pi\int f_1(x) + (1-\pi)\int f_2(x) = \pi+1-\pi=1$
```{marginfigure}
$\blacksquare$
```

## b

------------------------------------------------------------------

The distribution function is just the integral of the density. Therefore

$G(x) = \pi F_1(x) + (1-\pi)F_2(x)$

Where $G(x)$ is thee distribution function associated with $g$


## c

------------------------------------------------------------------

The survival function is simply 1-cdf, so

$1 - G(x) =1 - (\pi F_1(x) + (1-\pi)F_2(x))$

Which can be simplified to 

$1 - G(x) =1 - \pi F_1(x) - F_2(x) + \pi F_2(x)$

And by substituting $1-F$ for $S$,

$S_g(x) =(1+\pi)S_2(x) + (2-\pi)S_1(x)$


# Problem 8

------------------------------------------------------------------

## a

------------------------------------------------------------------

$e^{-x}$ is always positive, so for any given value this function is > 0. 

To find if this function is < 1 at its maximum, one way to go about doing this is taking the derivative and finding local maxima. 

$(1+e^{-x})^2(-e^-x)-e^{-x}(2(1+e^{-x})(-e^{-x}))/(1+e^{-x})^4 = 0$

$(1+e^{-x})^2(-e^-x)= e^{-x}(2(1+e^{-x})(-e^{-x}))$

$1+e^{-x}= 2e^{-x}$

$1= e^{-x}$

We see this is true at (and only at) $x=0$ 

The value of the function at $x=0$ is

$e^0/(1+e^0)^2= 1/(1+1)^2 = 0.25$

We can verify this by plotting:


```{r}
x <- seq(-10, 10, by = 0.1)
y <- function(x){
  exp(-x)/((1+exp(-x))^2)
}

plot(x, y(x))
```

Finally we can integrate to ensure that the area under the curve = 1. By substitution where $a = 1 + e^{-x}$, we see that the integral evaluates to

$(1+e^{-x})^{-1}|^\infty_{-\infty} = 1 - 0 = 1$
```{marginfigure}
$\blacksquare$
```
## b

------------------------------------------------------------------

The cdf is simply the integral of the density from its lower bound to some value x:

$(1+e^{-x})^{-1}|^x{-\infty}$

$(1+e^{-x})^{-1}$

## c

------------------------------------------------------------------

Plugging in 0 into the distribution equation yields 1, which in the context of this problem may mean 'having a score of 0 of this particular risk factor or this diagnostic readout leads to a completely certain chance of cancer'.

## d

------------------------------------------------------------------

The $p^th$ quantile is defined as the point on the cdf where $F(x) = p$. We also know that the cdf is $(1+e^{-x})^{-1}$. 

So, solving for x:

$(1+e^{-x})^{-1} = p$
$1+e^{-x} = 1/p$
$e^{-x} = $
$-x = log((1-p)/p)$
$x = log(((1-p)/p)^{-1}) = log(p/(1-p))$
```{marginfigure}
$\blacksquare$
```


# Problem 9

------------------------------------------------------------------

## a

------------------------------------------------------------------

The probability density is just the derivative. Given $x_0$ and $\alpha$ are constant, the pdf is

$\alpha(x_0/x)^{a-1}(x^{-2})$

or more simply

$\alpha x_0^\alpha/x^{\alpha+1}$

## b

------------------------------------------------------------------

```{r}

make_pareto_cdf <- function(x, x0, a) {
  if (x < x0) {
    0
  }
  else {
    1 - (x0/x)^a
  }
}

make_pareto_pdf <- function(x, x0, a) {
  if (x < x0) {
    0
  }
  else {
    a*x0^a/x^(a+1)
  }
}

pareto <- expand_grid(x = seq(0,110, 0.1), x0 = c(1, 2, 5), a = c(0.1, 1, 10)) %>% 
  mutate(y_cdf = unlist(pmap(list(x, x0, a), make_pareto_cdf)),
         y_pdf = unlist(pmap(list(x, x0, a), make_pareto_pdf))) %>% 
  pivot_longer(cols = c(y_cdf, y_pdf), names_to = "fun")

ggplot(pareto, aes(x = x, y = value, color = fun)) + geom_line() + facet_grid(a~x0) + coord_cartesian(ylim = c(0, 1), xlim = c(0, 10))

```
The scale parameter seems to shift the beginning of the function (likely relating to the 'base' number of years a product is expected to last before failing) while the shape parameter seems inversely related to the sharpness of the pdf peak - likely relating to the onset of failure after the given 'due date'.

## c

------------------------------------------------------------------

We know that by feeding samples from a uniform distribution into the inverse of the CDF for the Pareto distribution we can get numbers that are similarly distributed as to those sampled from a random variable with a distribution resembling the Pareto distribution.

$x = 1-(x_0/F^{-1}(x))^\alpha$
$(x_0/F^{-1}(x))^\alpha= 1-x$
$log(x_0/F^{-1}(x))= log(1-x)/\alpha$
$log(x_0) - log(F^{-1}(x))= log(1-x)/\alpha$
$log(F^{-1}(x))= log(x_0) - log(1-x)^{1/\alpha}$
$F^{-1}(x)= x_0/(1-x)^{1/\alpha}$

```{r}
x <- runif(10000)
y <- function(x, x0, a){
  (x0/(1-x))^(1/a)
}

pareto <- tibble(x, y = y(x, 1, 2))


ggplot(pareto, aes(x = y)) + geom_histogram(binwidth = .1) + coord_cartesian(xlim = c(0, 10))

```

## d

------------------------------------------------------------------

Survival = 1-cdf

$S(x) = \{^{(x_0/x)^\alpha}_{1}$

For values $x = 10$ years, $\alpha = 1$, and $x_0 = 2$,

$S(10) = (2/10)^1 = 1/5$

This could be interpreted as the likelihood that a given part will still be functioning at 10 years given an expected lifespan of 2 years

## e

------------------------------------------------------------------

We previously found the inverse of the cdf, which is the quantile function:

$F^{-1}(x)= x_0/(1-x)^{1/\alpha}$ 

With an input of $p = .8$, this function would return the number of years that it would be expected that 80% of units have failed.


# Problem 10

------------------------------------------------------------------

## a

------------------------------------------------------------------

If this is to be a valid density, the integral from 0 to 1 should be 1. That is,

$\int^1_0cx^k=1$
$c(x^{k+1}/(k+1)|^1_0)=1$
$c/(k+1)=1$
$c=k+1$

## b

------------------------------------------------------------------

The cdf is just the integral from the lower bound to x, so 

$F(x) = \int^x_0cx^k$
$F(x) = c(x^{k+1}/(k+1)|^x_0)$
$F(x) = cx^{k+1}/(k+1)$

and since we know $c=k+1$, 

$F(x) = x^{k+1}$


## c

------------------------------------------------------------------

The quantile formula is the inverse cdf, so

$x= F^{-1}(x)^{k+1}$

$log(x)= (k+1)log(F^{-1}(x))$

$log(F^{-1}(x)) = log(x)/(k+1)$

$log(F^{-1}(x)) = log(x^{1/(k+1)})$

$F^{-1}(x) = x^{1/(k+1)}$


## d

------------------------------------------------------------------

This is simply the cdf where the lower bound is $a$ and the upper bound is $b$, so

$F(x) = c(x^{k+1}/(k+1)|^b_a)$

$F(x) = (x^{k+1}|^b_a)$

$F(x) = b^{k+1}-a^{k+1}$


# Problem 11

------------------------------------------------------------------

## a

------------------------------------------------------------------

To make this a valid density, the following must be true:

$\int_0^\infty f(x) = 1$

$\int_0^\infty ce^{-x/2.5} = 1$

$-2.5c(e^{-x/2.5}|^\infty_0)=1$

$-2.5c(0-1)=1$

$c = 1/2.5$


## b

------------------------------------------------------------------

This is the integral of the pdf from the lower bound (0) to x:

$\int_0^x f(x)$

$\int_0^x e^{-x/2.5}/2.5$

$-(e^{-x/2.5}-1)$

$F(x) = 1-e^{-x/2.5}$

## c

------------------------------------------------------------------

The survival function is  1-cdf or

$S(x) = 1-(1-e^{-x/2.5})$

$S(x) = e^{-x/2.5}$


## d

------------------------------------------------------------------

$S(11) = e^{-11/2.5} \approx 0.012$


## e

------------------------------------------------------------------

This involves finding the quantile function, which is the inverse of the cdf:

$x = 1-e^{-F^{-1}(x)/2.5}$

$1-x = e^{-F^{-1}(x)/2.5}$

$log(1-x) = -F^{-1}(x)/2.5$

$F^{-1}(x) = -2.5log(1-x)$

And then find the value where $x = 0.5$

$F^{-1}(0.5) = -2.5log(.5) \approx 1.73$

